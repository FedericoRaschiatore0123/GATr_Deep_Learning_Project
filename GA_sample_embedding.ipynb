{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedericoRaschiatore0123/GATr_Deep_Learning_Project/blob/main/GA_sample_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzmo6UGthpRf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary of global variables\n",
        "global_variables = {\n",
        "\n",
        "    'dim_GA': 16\n",
        "}"
      ],
      "metadata": {
        "id": "CbnoAp1SixuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geometric Algebra Embedding\n",
        "\n",
        "Geometric algebra G_{3,0,1} is characterized by 4 basic elements (e_{0}, e_{1}, e_{2}, e_{3}) that represent planes in the 3D space. These 4 basic elements can be combined through linear combination and geometric transformation to represent geometric objects of different dimensions (planes, lines, points), and transformations of different types (reflections, translations, rotations).\n",
        "\n",
        "G_{3,0,1} indicates that the basis is composed by three elements (e_{1}, e_{2}, e_{3}) that satisfy e_{i}e_{i} = 1 and one (e_{0}) that satisfies e_{0}e_{0} = 0.\n",
        "\n",
        "The basis components of the multivector associated to the G_{3,0,1} algebra are obtained by considering every possible multiplicative combination of the basis elements. Elements of different grade are thus obtained:\n",
        "\n",
        "Grade 0 (1): scalars in the G.A. \\\\\n",
        "Grade 1 (e_{0}, e_{i}): vectors in the G.A. \\\\\n",
        "Grade 2 (e_{0i}, e_{ij}); bivectors in the G.A. \\\\\n",
        "Grade 3 (e_{0ij}, e_{ijk}): trivectors in the G.A. \\\\\n",
        "Grade 4 (e_{0123}): pseudoscalars in the G.A. \\\\\n",
        "\n",
        "The multivector representation associated to the Geometric Algebra G_{3,0,1} will thus be composed of a total of 16 elements.\n",
        "\n",
        "Scalars will be represented by grade 0 elements, planes by grade 1 elements, lines by grade 2 elements, points by grade 3 elements and pseudoscalar by grade 4 elements."
      ],
      "metadata": {
        "id": "utHHBu-PoH-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_pos_mv_16(pos):\n",
        "    \"\"\"\n",
        "    Embeds the pos torch tensor as a 16-dimensional G_{3,0,1} multivector\n",
        "\n",
        "    Args:\n",
        "        pos (torch.Tensor): tensor of input points with shape (n_elements, 3)\n",
        "\n",
        "    Returns:\n",
        "        mv torch.Tensor: tensor of multivectors of dimension 16 with shape (1, n_elements, 1, 16)\n",
        "    \"\"\"\n",
        "    # Positions in Euclidean geometry can be embedded as points in G_{3,0,1} algebra\n",
        "\n",
        "    # Get the number of points and the dimensionality\n",
        "    n_elements = pos.shape[0]\n",
        "    dim = pos.shape[1]\n",
        "\n",
        "    # Create multivector tensor\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "\n",
        "    multivector[:, 14] = 1 # homogeneous component, e_{123}\n",
        "    multivector[:, 11] = pos[:, 0] # x, e_{023}\n",
        "    multivector[:, 12] = pos[:, 1] # y, e_{013}\n",
        "    multivector[:, 13] = pos[:, 2] # z, e_{012}\n",
        "\n",
        "    mv = multivector.reshape(1,n_elements,1,global_variables['dim_GA'])\n",
        "\n",
        "    return mv\n",
        "\n",
        "\n",
        "# ???: question about sign convention\n",
        "# the choice of the sign allow to give the correct geometric interpretation\n",
        "#       to the components. Important to have the convention be consistent\n",
        "#       for the whole embedding"
      ],
      "metadata": {
        "id": "qzONXL64kU3D"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_face_mv_16(face):\n",
        "    \"\"\"\n",
        "    Embeds the face torch tensor as a 16-dimensional G_{3,0,1} multivector\n",
        "\n",
        "    Input:\n",
        "        face torch.Tensor of size (n_elements, 3)\n",
        "    Output:\n",
        "        mv torch.Tensor of size (1, n_elements, 1, 16)\n",
        "    \"\"\"\n",
        "\n",
        "    # An oriented surface in Euclidean geometry characterized by\n",
        "    #   the normal vector n to the surface itself can be embedded as an oriented\n",
        "    #   plane in G_{3,0,1} algebra\n",
        "\n",
        "\n",
        "    # Get the number of points and the dimensionality of the space\n",
        "    n_elements = face.shape[0]\n",
        "    dim = face.shape[1]\n",
        "\n",
        "    # Create multivector tensor\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "\n",
        "    multivector[:, 2] = face[:, 0] # e_{1}\n",
        "    multivector[:, 3] = face[:, 1] # e_{2}\n",
        "    multivector[:, 4] = face[:, 2] # e_{3}\n",
        "\n",
        "    mv = multivector.reshape(1,n_elements,1,global_variables['dim_GA'])\n",
        "\n",
        "    return mv"
      ],
      "metadata": {
        "id": "wWOSDYn0kiHL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_wss_mv_16(wss):\n",
        "    \"\"\"\n",
        "    Embeds the wss vectors as a 16 dimensional G_{3,0,1} multivector\n",
        "    Input:\n",
        "      wss torch.Tensor of size (n_elements, 3)\n",
        "    Output:\n",
        "      mv torch.Tensor of size (1, n_elements, 1, 16)\n",
        "    \"\"\"\n",
        "\n",
        "    # A 3D vector in the Euclidean space can be embedded as a translation in G_{3,0,1} algebra\n",
        "\n",
        "    n_elements = wss.shape[0]\n",
        "    dim = wss.shape[1]\n",
        "\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "\n",
        "    multivector[:, 0] = 1\n",
        "    multivector[:, 5] = 0.5*wss[:, 0] # e_{01}\n",
        "    multivector[:, 6] = 0.5*wss[:, 1] # e_{02}\n",
        "    multivector[:, 7] = 0.5*wss[:, 2] # e_{03}\n",
        "\n",
        "    mv = multivector.reshape(1,n_elements,1,global_variables['dim_GA'])\n",
        "\n",
        "    return mv"
      ],
      "metadata": {
        "id": "ZU50dY0jkjOL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_inlet_mv_16(inlet):\n",
        "    \"\"\"\n",
        "    Embeds the inlet torch tensor as a 16 dimensional G_{3,0,1} multivector\n",
        "    Input:\n",
        "        inlet torch.Tensor of size (n_elements, 1)\n",
        "    Output:\n",
        "        mv torch.Tensor of size (1, n_elements, 1, 16)\n",
        "    \"\"\"\n",
        "    # Inlet indexes can be embedded as scalars in G_{3,0,1} algebra\n",
        "\n",
        "    n_elements = inlet.shape[0]\n",
        "\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "    multivector[:, 0] = inlet[:]\n",
        "\n",
        "    mv = multivector.reshape(1, n_elements, 1, global_variables['dim_GA'])\n",
        "\n",
        "    return mv"
      ],
      "metadata": {
        "id": "G75y3a4wknYV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_pressure_mv_16(pressure):\n",
        "  \"\"\"\n",
        "  Embeds the pressure torch tensor as a 16-dimensional G_{3,0,1} multivector\n",
        "  Input:\n",
        "    pressure torch.Tensor of size (n_elements, 1)\n",
        "  Output:\n",
        "    mv torch.Tensor of size (1, n_elements, 1, 16)\n",
        "  \"\"\"\n",
        "\n",
        "  n_elements = pressure.shape[0]\n",
        "\n",
        "  multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "  multivector[:,0] = pressure[:]\n",
        "\n",
        "  mv = multivector.reshape(1, n_elements, 1, global_variables['dim_GA'])\n",
        "\n",
        "  return mv"
      ],
      "metadata": {
        "id": "M7Jj-SMfVkR3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_complete_mv_16(sample_path):\n",
        "  # passa alla funzione il sample path,\n",
        "  # fagli estrarre i tensori in torch,\n",
        "  # fai l'embedding per ogni componente\n",
        "  # e infine concatena tutti gli embedding\n",
        "\n",
        "  # ritorna il multivector completo per il sample con path sample_path\n",
        "\n",
        "  with h5py.File(sample_path, 'r') as f:\n",
        "    # extract torch tensor for pos\n",
        "    pos_data = f['pos'][:50]\n",
        "    pos_torch = torch.tensor(pos_data[:])\n",
        "\n",
        "    # extract torch tensor for face\n",
        "    face_data = f['face'][:50]\n",
        "    face_torch = torch.tensor(face_data[:])\n",
        "\n",
        "    # extract torch tensor for wss\n",
        "    wss_data = f['wss'][:50]\n",
        "    wss_torch = torch.tensor(wss_data[:])\n",
        "\n",
        "    # extract torch tensor for pressure\n",
        "    pressure_data = f['pressure'][:50]\n",
        "    pressure_torch = torch.tensor(pressure_data[:])\n",
        "\n",
        "    # extract torch tensor for inlet_idcs\n",
        "    inlet_data = f['inlet_idcs'][:50]\n",
        "    inlet_torch = torch.tensor(inlet_data[:])\n",
        "\n",
        "  # Get the multivector embeddings for each torch tensor\n",
        "  pos_mv_16 = embed_pos_mv_16(pos_torch)\n",
        "  face_mv_16 = embed_face_mv_16(face_torch)\n",
        "  wss_mv_16 = embed_wss_mv_16(wss_torch)\n",
        "  inlet_mv_16 = embed_inlet_mv_16(inlet_torch)\n",
        "  pressure_mv_16 = embed_pressure_mv_16(pressure_torch)\n",
        "\n",
        "  sample_embedding = torch.cat(\n",
        "      [pos_mv_16, face_mv_16, wss_mv_16, inlet_mv_16, pressure_mv_16], dim = 2)\n",
        "\n",
        "  return sample_embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D4VeD5dlSoqN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use of embed_complete_mv_16\n",
        "sample_path = \"/content/sample_0000.hdf5\"\n",
        "sample_embedding = embed_complete_mv_16(sample_path)\n",
        "print(sample_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2zhjVaHZl7Z",
        "outputId": "4fa49284-286f-4684-822c-f7f1571cc191"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50, 5, 16])\n"
          ]
        }
      ]
    }
  ]
}