{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSDYQ18EI9GvbXkjG7dkXg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedericoRaschiatore0123/GATr_Deep_Learning_Project/blob/main/GA_sample_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kzmo6UGthpRf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary of global variables\n",
        "global_variables = {\n",
        "\n",
        "    'dim_GA': 16\n",
        "}"
      ],
      "metadata": {
        "id": "CbnoAp1SixuX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_pos_mv_16(pos):\n",
        "    \"\"\"\n",
        "    Embeds points in 3D space into multivectors of dimension 16 using an alternative approach.\n",
        "\n",
        "    Args:\n",
        "        pos (torch.Tensor): tensor of input points with shape (n_elements, 3)\n",
        "\n",
        "    Returns:\n",
        "        mv torch.Tensor: tensor of multivectors of dimension 16 with shape (1, n_elements, 1, 16)\n",
        "    \"\"\"\n",
        "    # Get the number of points and the dimensionality\n",
        "    n_elements, dim = pos.shape\n",
        "\n",
        "    # Create multivector tensor\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "\n",
        "    multivector[:, 14] = 1 # homogeneous component, e_{123}\n",
        "    multivector[:, 11] = pos[:, 0] # x, e_{023}\n",
        "    multivector[:, 12] = pos[:, 1] # y, e_{013}\n",
        "    multivector[:, 13] = pos[:, 2] # z, e_{012}\n",
        "\n",
        "    mv = multivector.reshape(1,n_elements,1,global_variables['dim_GA'])\n",
        "\n",
        "    return mv\n",
        "\n",
        "\n",
        "# ???: question about sign convention\n",
        "# sign convention chosen as indicated in the paper\n",
        "# the choice of the sign allow to give the correct geometric interpretation\n",
        "#       to the components. Important to have the convention be consistent\n",
        "#       for the whole embedding"
      ],
      "metadata": {
        "id": "qzONXL64kU3D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check function!\n",
        "pts = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15],\n",
        "             [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27], [28, 29, 30]]\n",
        "pts_tensor = torch.tensor(pts)\n",
        "mv = embed_pos_mv_16(pts_tensor)\n",
        "print(mv) # ok!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0aIiU9fkV41",
        "outputId": "4f5e9659-51c2-47f6-f8ed-8231ed730216"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  3.,\n",
            "            1.,  0.]],\n",
            "\n",
            "         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,  5.,  6.,\n",
            "            1.,  0.]],\n",
            "\n",
            "         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  7.,  8.,  9.,\n",
            "            1.,  0.]],\n",
            "\n",
            "         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 11., 12.,\n",
            "            1.,  0.]],\n",
            "\n",
            "         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 13., 14., 15.,\n",
            "            1.,  0.]],\n",
            "\n",
            "         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 16., 17., 18.,\n",
            "            1.,  0.]],\n",
            "\n",
            "         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 19., 20., 21.,\n",
            "            1.,  0.]],\n",
            "\n",
            "         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 22., 23., 24.,\n",
            "            1.,  0.]],\n",
            "\n",
            "         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 25., 26., 27.,\n",
            "            1.,  0.]],\n",
            "\n",
            "         [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 28., 29., 30.,\n",
            "            1.,  0.]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_face_mv_16(face):\n",
        "    \"\"\"\n",
        "    Embeds the faces into multivectors of dimension 16\n",
        "\n",
        "    Input:\n",
        "        face torch.Tensor of size (n_elements, 3)\n",
        "    Output:\n",
        "        mv torch.Tensor of size (1, n_elements, 1, 16)\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the number of points and the dimensionality\n",
        "    n_elements, dim = face.shape\n",
        "\n",
        "    # Create multivector tensor\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "\n",
        "    multivector[:, 2] = face[:, 0] # e_{1}\n",
        "    multivector[:, 3] = face[:, 1] # e_{2}\n",
        "    multivector[:, 4] = face[:, 2] # e_{3}\n",
        "\n",
        "    mv = multivector.reshape(1,n_elements,1,global_variables['dim_GA'])\n",
        "\n",
        "    return mv"
      ],
      "metadata": {
        "id": "wWOSDYn0kiHL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_wss_mv_16(wss):\n",
        "\n",
        "\n",
        "    # it can be embedded as a translation in the G_{3,0,1} space\n",
        "\n",
        "    n_elements, dim = wss.shape\n",
        "\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "\n",
        "    multivector[:, 0] = 1\n",
        "    multivector[:, 5] = 0.5*wss[:, 0] # e_{01}\n",
        "    multivector[:, 6] = 0.5*wss[:, 1] # e_{02}\n",
        "    multivector[:, 7] = 0.5*wss[:, 2] # e_{03}\n",
        "\n",
        "    mv = multivector.reshape(1,n_elements,1,global_variables['dim_GA'])\n",
        "\n",
        "    return mv"
      ],
      "metadata": {
        "id": "ZU50dY0jkjOL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_inlet_mv_16(inlet):\n",
        "    \"\"\"\n",
        "    Embed the inlet torch tensor as a 16 dimensional multivector\n",
        "    Input:\n",
        "        inlet torch.Tensor of size (n_elements, 1)\n",
        "    Output:\n",
        "        mv torch.Tensor of size (1, n_elements, 1, 1)\n",
        "    \"\"\"\n",
        "    n_elements = inlet.shape[0]\n",
        "\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "    multivector[:, 0] = inlet[:,0]\n",
        "\n",
        "    mv = multivector.reshape(1, n_elements, 1, global_variables['dim_GA'])\n",
        "\n",
        "    return mv"
      ],
      "metadata": {
        "id": "G75y3a4wknYV"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}