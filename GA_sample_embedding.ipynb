{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedericoRaschiatore0123/GATr_Deep_Learning_Project/blob/main/GA_sample_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzmo6UGthpRf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary of global variables\n",
        "global_variables = {\n",
        "\n",
        "    'dim_GA': 16\n",
        "}"
      ],
      "metadata": {
        "id": "CbnoAp1SixuX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geometric Algebra Embedding\n",
        "\n",
        "Geometric algebra G_{3,0,1} is characterized by 4 basic elements (e_{0}, e_{1}, e_{2}, e_{3}) that represent planes in the 3D space. These 4 basic elements can be combined through linear combination and geometric transformation to represent geometric objects of different dimensions (planes, lines, points), and transformations of different types (reflections, translations, rotations).\n",
        "\n",
        "G_{3,0,1} indicates that the basis is composed by three elements (e_{1}, e_{2}, e_{3}) that satisfy e_{i}e_{i} = 1 and one (e_{0}) that satisfies e_{0}e_{0} = 0.\n",
        "\n",
        "The basis components of the multivector associated to the G_{3,0,1} algebra are obtained by considering every possible multiplicative combination of the basis elements. Elements of different grade are thus obtained:\n",
        "\n",
        "Grade 0 (1): scalars in the G.A. \\\\\n",
        "Grade 1 (e_{0}, e_{i}): vectors in the G.A. \\\\\n",
        "Grade 2 (e_{0i}, e_{ij}); bivectors in the G.A. \\\\\n",
        "Grade 3 (e_{0ij}, e_{ijk}): trivectors in the G.A. \\\\\n",
        "Grade 4 (e_{0123}): pseudoscalars in the G.A. \\\\\n",
        "\n",
        "The multivector representation associated to the Geometric Algebra G_{3,0,1} will thus be composed of a total of 16 elements.\n",
        "\n",
        "Scalars will be represented by grade 0 elements, planes by grade 1 elements, lines by grade 2 elements, points by grade 3 elements and pseudoscalar by grade 4 elements."
      ],
      "metadata": {
        "id": "utHHBu-PoH-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_pos_mv_16(pos):\n",
        "    \"\"\"\n",
        "    Embeds the pos torch tensor as a 16-dimensional G_{3,0,1} multivector\n",
        "\n",
        "    Args:\n",
        "        pos (torch.Tensor): tensor of input points with shape (n_elements, 3)\n",
        "\n",
        "    Returns:\n",
        "        mv torch.Tensor: tensor of multivectors of dimension 16 with shape (1, n_elements, 1, 16)\n",
        "    \"\"\"\n",
        "    # Positions in Euclidean geometry can be embedded as points in G_{3,0,1} algebra\n",
        "\n",
        "    # Get the number of points and the dimensionality\n",
        "    n_elements = pos.shape[0]\n",
        "    dim = pos.shape[1]\n",
        "\n",
        "    # Create multivector tensor\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "\n",
        "    multivector[:, 14] = 1 # homogeneous component, e_{123}\n",
        "    multivector[:, 11] = pos[:, 0] # x, e_{023}\n",
        "    multivector[:, 12] = pos[:, 1] # y, e_{013}\n",
        "    multivector[:, 13] = pos[:, 2] # z, e_{012}\n",
        "\n",
        "    mv = multivector.reshape(1,n_elements,1,global_variables['dim_GA'])\n",
        "\n",
        "    return mv\n",
        "\n",
        "\n",
        "# ???: question about sign convention\n",
        "# the choice of the sign allow to give the correct geometric interpretation\n",
        "#       to the components. Important to have the convention be consistent\n",
        "#       for the whole embedding"
      ],
      "metadata": {
        "id": "qzONXL64kU3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_face_mv_16(face):\n",
        "    \"\"\"\n",
        "    Embeds the face torch tensor as a 16-dimensional G_{3,0,1} multivector\n",
        "\n",
        "    Input:\n",
        "        face torch.Tensor of size (n_elements, 3)\n",
        "    Output:\n",
        "        mv torch.Tensor of size (1, n_elements, 1, 16)\n",
        "    \"\"\"\n",
        "\n",
        "    # An oriented surface in Euclidean geometry characterized by\n",
        "    #   the normal vector n to the surface itself can be embedded as an oriented\n",
        "    #   plane in G_{3,0,1} algebra\n",
        "\n",
        "\n",
        "    # Get the number of points and the dimensionality of the space\n",
        "    n_elements = face.shape[0]\n",
        "    dim = face.shape[1]\n",
        "\n",
        "    # Create multivector tensor\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "\n",
        "    multivector[:, 2] = face[:, 0] # e_{1}\n",
        "    multivector[:, 3] = face[:, 1] # e_{2}\n",
        "    multivector[:, 4] = face[:, 2] # e_{3}\n",
        "\n",
        "    mv = multivector.reshape(1,n_elements,1,global_variables['dim_GA'])\n",
        "\n",
        "    return mv"
      ],
      "metadata": {
        "id": "wWOSDYn0kiHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_wss_mv_16(wss):\n",
        "    \"\"\"\n",
        "    Embeds the wss vectors as a 16 dimensional G_{3,0,1} multivector\n",
        "    Input:\n",
        "      wss torch.Tensor of size (n_elements, 3)\n",
        "    Output:\n",
        "      mv torch.Tensor of size (1, n_elements, 1, 16)\n",
        "    \"\"\"\n",
        "\n",
        "    # A 3D vector in the Euclidean space can be embedded as a translation in G_{3,0,1} algebra\n",
        "\n",
        "    n_elements = wss.shape[0]\n",
        "    dim = wss.shape[1]\n",
        "\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "\n",
        "    multivector[:, 0] = 1\n",
        "    multivector[:, 5] = 0.5*wss[:, 0] # e_{01}\n",
        "    multivector[:, 6] = 0.5*wss[:, 1] # e_{02}\n",
        "    multivector[:, 7] = 0.5*wss[:, 2] # e_{03}\n",
        "\n",
        "    mv = multivector.reshape(1,n_elements,1,global_variables['dim_GA'])\n",
        "\n",
        "    return mv"
      ],
      "metadata": {
        "id": "ZU50dY0jkjOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_inlet_mv_16(inlet):\n",
        "    \"\"\"\n",
        "    Embeds the inlet torch tensor as a 16 dimensional G_{3,0,1} multivector\n",
        "    Input:\n",
        "        inlet torch.Tensor of size (n_elements, 1)\n",
        "    Output:\n",
        "        mv torch.Tensor of size (1, n_elements, 1, 16)\n",
        "    \"\"\"\n",
        "    # Inlet indexes can be embedded as scalars in G_{3,0,1} algebra\n",
        "\n",
        "    n_elements = inlet.shape[0]\n",
        "\n",
        "    multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "    multivector[:, 0] = inlet[:]\n",
        "\n",
        "    mv = multivector.reshape(1, n_elements, 1, global_variables['dim_GA'])\n",
        "\n",
        "    return mv"
      ],
      "metadata": {
        "id": "G75y3a4wknYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_pressure_mv_16(pressure):\n",
        "  \"\"\"\n",
        "  Embeds the pressure torch tensor as a 16-dimensional G_{3,0,1} multivector\n",
        "  Input:\n",
        "    pressure torch.Tensor of size (n_elements, 1)\n",
        "  Output:\n",
        "    mv torch.Tensor of size (1, n_elements, 1, 16)\n",
        "  \"\"\"\n",
        "\n",
        "  n_elements = pressure.shape[0]\n",
        "\n",
        "  multivector = torch.zeros(n_elements, global_variables['dim_GA'])\n",
        "  multivector[:,0] = pressure[:]\n",
        "\n",
        "  mv = multivector.reshape(1, n_elements, 1, global_variables['dim_GA'])\n",
        "\n",
        "  return mv"
      ],
      "metadata": {
        "id": "M7Jj-SMfVkR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_complete_mv_16(sample_path):\n",
        "  # passa alla funzione il sample path,\n",
        "  # fagli estrarre i tensori in torch,\n",
        "  # fai l'embedding per ogni componente\n",
        "  # e infine concatena tutti gli embedding\n",
        "\n",
        "  # ritorna il multivector completo per il sample con path sample_path\n",
        "\n",
        "  with h5py.File(sample_path, 'r') as f:\n",
        "    # extract torch tensor for pos\n",
        "    pos_data = f['pos'][:50]\n",
        "    pos_torch = torch.tensor(pos_data[:])\n",
        "\n",
        "    # extract torch tensor for face\n",
        "    face_data = f['face'][:50]\n",
        "    face_torch = torch.tensor(face_data[:])\n",
        "\n",
        "    # extract torch tensor for wss\n",
        "    wss_data = f['wss'][:50]\n",
        "    wss_torch = torch.tensor(wss_data[:])\n",
        "\n",
        "    # extract torch tensor for pressure\n",
        "    pressure_data = f['pressure'][:50]\n",
        "    pressure_torch = torch.tensor(pressure_data[:])\n",
        "\n",
        "    # extract torch tensor for inlet_idcs\n",
        "    inlet_data = f['inlet_idcs'][:50]\n",
        "    inlet_torch = torch.tensor(inlet_data[:])\n",
        "\n",
        "  # Get the multivector embeddings for each torch tensor\n",
        "  pos_mv_16 = embed_pos_mv_16(pos_torch)\n",
        "  face_mv_16 = embed_face_mv_16(face_torch)\n",
        "  wss_mv_16 = embed_wss_mv_16(wss_torch)\n",
        "  inlet_mv_16 = embed_inlet_mv_16(inlet_torch)\n",
        "  pressure_mv_16 = embed_pressure_mv_16(pressure_torch)\n",
        "\n",
        "  sample_embedding = torch.cat(\n",
        "      [pos_mv_16, face_mv_16, wss_mv_16, inlet_mv_16, pressure_mv_16], dim = 2)\n",
        "\n",
        "  return sample_embedding\n"
      ],
      "metadata": {
        "id": "D4VeD5dlSoqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use of embed_complete_mv_16\n",
        "sample_path = \"/content/sample_0000.hdf5\"\n",
        "sample_embedding = embed_complete_mv_16(sample_path)\n",
        "print(sample_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2zhjVaHZl7Z",
        "outputId": "4fa49284-286f-4684-822c-f7f1571cc191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50, 5, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geometric Algebra Transformations"
      ],
      "metadata": {
        "id": "B1o5C5ZZkY8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plane reflection given normal to the plane and origin shift\n",
        "\n",
        "def embed_plane_reflection_mv_16(normal, d):\n",
        "  \"\"\"\n",
        "  Embeds a plane reflection transformation given a normal vector and an origin shift\n",
        "  as a 16-dimensional multivector in G_{3,0,1}\n",
        "  Inputs:\n",
        "    normal: torch.Tensor (1, 3) normal to the plane\n",
        "    d: origin shift\n",
        "  Output:\n",
        "    pr_mv_16: torch.Tensor multivector of size (1,1,1,16) representing the plane reflection\n",
        "  \"\"\"\n",
        "\n",
        "  plane_reflection_mv_16 = torch.zeros(1, global_variables['dim_GA'])\n",
        "  plane_reflection_mv_16[0,1] = d\n",
        "  plane_reflection_mv_16[0,2] = normal[0]\n",
        "  plane_reflection_mv_16[0,3] = normal[1]\n",
        "  plane_reflection_mv_16[0,4] = normal[2]\n",
        "\n",
        "  pr_mv_16 = plane_reflection_mv_16.reshape(1,1,1,global_variables['dim_GA'])\n",
        "\n",
        "  return pr_mv_16\n",
        "\n"
      ],
      "metadata": {
        "id": "UgzSRPgvf5Yv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translation by t in R^{3}\n",
        "def embed_translation_mv_16(t):\n",
        "  \"\"\"\n",
        "  Embeds a translation by a vector t in R^{3} as a 16-dimensional multivector in G_{3,0,1}\n",
        "  Inputs:\n",
        "    t: torch.Tensor of size (1, 3) representing a translation in the 3D space\n",
        "  Outputs:\n",
        "    tr_mv_16: torch.Tensor multivector of size (1,1,1,16) representing the translation\n",
        "  \"\"\"\n",
        "  translation_mv_16 = torch.zeros(1, global_variables['dim_GA'])\n",
        "\n",
        "  translation_mv_16[0,0] = 1 # homogeneous component\n",
        "  translation_mv_16[0,5] = 0.5*t[0]\n",
        "  translation_mv_16[0,6] = 0.5*t[1]\n",
        "  translation_mv_16[0,7] = 0.5*t[2]\n",
        "\n",
        "  tr_mv_16 = translation_mv_16.reshape(1,1,1,global_variables['dim_GA'])\n",
        "\n",
        "  return tr_mv_16\n",
        "\n"
      ],
      "metadata": {
        "id": "ULbwfw47kUWv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([2, 4, 6])\n",
        "d = 4\n",
        "translation_mv = embed_translation_mv_16(t)\n",
        "\n",
        "print(translation_mv)"
      ],
      "metadata": {
        "id": "w9kZg5b_ibV5",
        "outputId": "0a3ba082-e9ad-4d39-c787-2ecb6f68a8b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1., 0., 0., 0., 0., 1., 2., 3., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### prova blade\n",
        "\n",
        "mv_dimension = 16\n",
        "grade_components = [1, 4, 6, 4, 1]\n",
        "blade_shape = (mv_dimension,mv_dimension) # (16,16)\n",
        "\n",
        "coordinates = []\n",
        "start = 0\n",
        "for length in grade_components:\n",
        "    coordinates.append(list(range(start, start + length)))\n",
        "    start += length\n",
        "print(coordinates)\n"
      ],
      "metadata": {
        "id": "nLG3nh7f-zRM",
        "outputId": "5f25b678-0305-4e4d-9853-e7148899955f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0], [1, 2, 3, 4], [5, 6, 7, 8, 9, 10], [11, 12, 13, 14], [15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_dimension = len(grade_components)\n",
        "print(w_dimension) # 5 (number of blades)\n"
      ],
      "metadata": {
        "id": "u8Lvj60V_6ge",
        "outputId": "dc108ace-00b4-43f7-f471-299e835720a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "blade_mask = []\n",
        "\n",
        "w_dimension = len(grade_components) # 5\n",
        "for k_grade in range(w_dimension):\n",
        "    print('k grade')\n",
        "    print(k_grade)\n",
        "    w_blade = torch.zeros(blade_shape) # per ogni grado di k genero una w_blade\n",
        "    for coordinate in coordinates[k_grade]:\n",
        "        w_blade[coordinate, coordinate] = 1.0\n",
        "    blade_mask.append(w_blade.unsqueeze(0))\n",
        "print(blade_mask) # sono 5 matrici 16 x 16 (una per ogni blade), ciascuna ha elementi =1 sulla diagonale maggiore in corrispondenza degli indici della blade!!!"
      ],
      "metadata": {
        "id": "XTl0MYCAP_2N",
        "outputId": "b3d35487-18fd-4b13-dac4-b78ef9219f21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k grade\n",
            "0\n",
            "k grade\n",
            "1\n",
            "k grade\n",
            "2\n",
            "k grade\n",
            "3\n",
            "k grade\n",
            "4\n",
            "[tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coord_permutations = [\n",
        "        [[0,1]],\n",
        "        [[2,5],[3,6],[4,7]],\n",
        "        [[8,11],[9,12],[10,13]],\n",
        "        [[14,15]]\n",
        "    ]\n",
        "\n",
        "v_dimension = len(grade_components) - 1\n",
        "for k_grade in range(v_dimension):\n",
        "    v_blade = torch.zeros(blade_shape) # 16 x 16\n",
        "    for coord_to,coord_from in coord_permutations[k_grade]: # 1,2,3,4\n",
        "        print('coord_from: ', coord_from)\n",
        "        print('coord_to: ',coord_to)\n",
        "        v_blade[coord_from, coord_to] = 1.0\n",
        "    print('end internal for')\n",
        "    blade_mask.append(v_blade.unsqueeze(0))\n",
        "\n",
        "blade_operator = torch.cat(blade_mask,dim = 0)"
      ],
      "metadata": {
        "id": "Wr_uDtUEQFig",
        "outputId": "e0a25311-8f93-44dd-f486-dde7863428f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coord_from:  1\n",
            "coord_to:  0\n",
            "end internal for\n",
            "coord_from:  5\n",
            "coord_to:  2\n",
            "coord_from:  6\n",
            "coord_to:  3\n",
            "coord_from:  7\n",
            "coord_to:  4\n",
            "end internal for\n",
            "coord_from:  11\n",
            "coord_to:  8\n",
            "coord_from:  12\n",
            "coord_to:  9\n",
            "coord_from:  13\n",
            "coord_to:  10\n",
            "end internal for\n",
            "coord_from:  15\n",
            "coord_to:  14\n",
            "end internal for\n"
          ]
        }
      ]
    }
  ]
}